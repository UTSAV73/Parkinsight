# -*- coding: utf-8 -*-
"""AutoencoderGridSearch_RP-new_new.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1bzs1Votqj7NW7OgkFM91EaL7BRADp6jU
"""

from google.colab import drive
drive.mount('/content/drive', force_remount=True)

import pandas as pd
import tensorflow as tf
feature_matrix = pd.read_csv("/content/circular_fm_new_pd_vs_swedd_V3.csv")

dfXY = feature_matrix[list(feature_matrix.columns[2:-1])].iloc[:-1]
dfXY

X = dfXY.values

Y = feature_matrix[["Class"]].iloc[:-1]
Y = Y.values
Y = Y.reshape(Y.shape[0],)

print(X.shape)
print(Y.shape)

X

import numpy as np
# Calculate class weights based on the class frequencies
Y = np.array([int(i) for i in list(Y)])
class_counts = np.bincount(Y)
total_samples = len(Y)
class_weights = total_samples / (len(class_counts) * class_counts)
scale_pos_weight = sum(class_weights) / len(class_weights)

initializer = tf.keras.initializers.glorot_uniform(seed=12)

"""# AUTOENCODER FOR CONTROL"""

# Train the autoencoder for class 0
from tensorflow.keras.optimizers import Adam


class_0_indices = Y == 0
X_class_0 = X[class_0_indices]
print(len(X_class_0))

"""Batch size = 16 , Epochs = 1000"""

# Define an autoencoder model with 4 dense layers for dimensionality reduction
autoencoder_0 = tf.keras.models.Sequential([
    tf.keras.layers.Dense(16, activation='relu', input_shape=(X.shape[1],), kernel_initializer=initializer),
    tf.keras.layers.BatchNormalization(),
    tf.keras.layers.Dense(8, activation='relu',kernel_initializer=initializer),
    tf.keras.layers.BatchNormalization(),
    tf.keras.layers.Dense(16, activation='relu',kernel_initializer=initializer),
    tf.keras.layers.BatchNormalization(),
    tf.keras.layers.Dense(X.shape[1], activation='linear',kernel_initializer=initializer)
])

# Set the learning rate for the optimizer
learning_rate=0.001

# Compile the autoencoder model with Mean Squared Error (MSE) loss and Adam optimizer
autoencoder_0.compile(optimizer=Adam(learning_rate=learning_rate), loss='mse')

# Train the autoencoder model on the data X_class_0
autoencoder_0.fit(X_class_0, X_class_0, epochs=1000, batch_size=16)

import matplotlib.pyplot as plt

plt.figure()
# Plot the loss values from the training history of the autoencoder model
plt.plot(autoencoder_0.history.history['loss'])
plt.show()

"""Comparision between Batch Size = 16 and 32"""

import matplotlib.pyplot as plt

# Define the batch sizes to train
batch_sizes = [16, 32]

# Define the colors for each batch size
colors = ['blue', 'red']

# Create a new figure for the plot
plt.figure()

for batch_size, color in zip(batch_sizes, colors):
    # Create the autoencoder model
    autoencoder_0 = tf.keras.models.Sequential([
        tf.keras.layers.Dense(16, activation='relu', input_shape=(X.shape[1],), kernel_initializer=initializer),
        tf.keras.layers.BatchNormalization(),
        tf.keras.layers.Dense(8, activation='relu', kernel_initializer=initializer),
        tf.keras.layers.BatchNormalization(),
        tf.keras.layers.Dense(16, activation='relu', kernel_initializer=initializer),
        tf.keras.layers.BatchNormalization(),
        tf.keras.layers.Dense(X.shape[1], activation='linear', kernel_initializer=initializer)
    ])

    learning_rate = 0.001

    autoencoder_0.compile(optimizer=Adam(learning_rate=learning_rate), loss='mse')

    # Train the autoencoder
    history = autoencoder_0.fit(X_class_0, X_class_0, epochs=1000, batch_size=batch_size)

    # Plot the loss values with the corresponding color and label
    plt.plot(history.history['loss'], color=color, label=f'Batch Size: {batch_size}')

# Add labels and legend to the plot
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.title('Loss vs. Epochs')
plt.legend()

# Show the plot
plt.show()

"""Batch size = 32 , Epochs = 1000"""

# Define the architecture of the autoencoder model
autoencoder_0 = tf.keras.models.Sequential([
    tf.keras.layers.Dense(16, activation='relu', input_shape=(X.shape[1],), kernel_initializer=initializer),
    tf.keras.layers.BatchNormalization(),
    tf.keras.layers.Dense(8, activation='relu',kernel_initializer=initializer),
    tf.keras.layers.BatchNormalization(),
    tf.keras.layers.Dense(16, activation='relu',kernel_initializer=initializer),
    tf.keras.layers.BatchNormalization(),
    tf.keras.layers.Dense(X.shape[1], activation='linear',kernel_initializer=initializer)
])

# Set the learning rate for the optimizer
learning_rate=0.001

# Compile the autoencoder model with Mean Squared Error (MSE) loss and Adam optimizer
autoencoder_0.compile(optimizer=Adam(learning_rate=learning_rate), loss='mse')

# Train the autoencoder model on the data X_class_0 with a batch size of 32 and for 1000 epochs
autoencoder_0.fit(X_class_0, X_class_0, epochs=1000, batch_size=32)

import matplotlib.pyplot as plt

plt.figure()
plt.plot(autoencoder_0.history.history['loss'])
plt.show()

"""# AUTOENCODER FOR PD

Comparision between Batch Size = 16 and 32
"""

class_1_indices = Y == 1
X_class_1 = X[class_1_indices]
print(len(X_class_1))

import matplotlib.pyplot as plt

# Define the batch sizes to train
batch_sizes = [16, 32, 64]

# Define the colors for each batch size
colors = ['blue', 'red','green']

# Create a new figure for the plot
plt.figure()

for batch_size, color in zip(batch_sizes, colors):
    # Create the autoencoder model
    autoencoder_1 = tf.keras.models.Sequential([
        tf.keras.layers.Dense(16, activation='relu', input_shape=(X.shape[1],), kernel_initializer=initializer),
        tf.keras.layers.BatchNormalization(),
        tf.keras.layers.Dense(8, activation='relu', kernel_initializer=initializer),
        tf.keras.layers.BatchNormalization(),
        tf.keras.layers.Dense(16, activation='relu', kernel_initializer=initializer),
        tf.keras.layers.BatchNormalization(),
        tf.keras.layers.Dense(X.shape[1], activation='linear', kernel_initializer=initializer)
    ])

    learning_rate = 0.001

    autoencoder_1.compile(optimizer=Adam(learning_rate=learning_rate), loss='mse')

    # Train the autoencoder
    history = autoencoder_1.fit(X_class_1, X_class_1, epochs=500, batch_size=batch_size)

    # Plot the loss values with the corresponding color and label
    plt.plot(history.history['loss'], color=color, label=f'Batch Size: {batch_size}')

# Add labels and legend to the plot
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.title('Loss vs. Epochs')
plt.legend()

# Show the plot
plt.show()

"""Fixing batch size = 64"""

from tensorflow.keras.optimizers import Adam

# Train the autoencoder for class 1

# Retrieve the indices of class 1 from the target variable Y
class_1_indices = Y == 1

# Select the corresponding data points for class 1 from the input variable X
X_class_1 = X[class_1_indices]

# Print the number of data points in class 1
print(len(X_class_1))

# Create the autoencoder model for class 1
autoencoder_1 = tf.keras.models.Sequential([
    tf.keras.layers.Dense(16, activation='relu', input_shape=(X.shape[1],), kernel_initializer=initializer),
    tf.keras.layers.BatchNormalization(),
    tf.keras.layers.Dense(8, activation='relu', kernel_initializer=initializer),
    tf.keras.layers.BatchNormalization(),
    tf.keras.layers.Dense(16, activation='relu', kernel_initializer=initializer),
    tf.keras.layers.BatchNormalization(),
    tf.keras.layers.Dense(X.shape[1], activation='linear', kernel_initializer=initializer)
])

# Set the learning rate for the optimizer
learning_rate = 0.001

# Compile the autoencoder model for class 1 with Mean Squared Error (MSE) loss and Adam optimizer
autoencoder_1.compile(optimizer=Adam(learning_rate=learning_rate), loss='mse')

# Train the autoencoder model for class 1 on the data X_class_1 with a batch size of 64 and for 500 epochs
autoencoder_1.fit(X_class_1, X_class_1, epochs=500, batch_size=64)

import matplotlib.pyplot as plt

plt.figure()
plt.plot(autoencoder_1.history.history['loss'])
plt.show()

"""Working with AE embeddings for Control and PD patients"""

from tensorflow.keras import Sequential

# Create an encoder model for autoencoder_0 by extracting its first four layers
encoder_0 = Sequential(autoencoder_0.layers[:4])

# Print the layers of encoder_0
print(encoder_0.layers)

# Print the output shape of the last layer in encoder_0
print(encoder_0.layers[-1].output_shape)

# Create an encoder model for autoencoder_1 by extracting its first four layers
encoder_1 = Sequential(autoencoder_1.layers[:4])

# Print the layers of encoder_1
print(encoder_1.layers)

# Print the output shape of the last layer in encoder_1
print(encoder_1.layers[-1].output_shape)

# FOR EMBEDDING CONTROL
X_Control_Encoded = encoder_0.predict(X_class_0)
print(X_Control_Encoded.shape)

# FOR EMBEDDING PD
X_PD_Encoded = encoder_1.predict(X_class_1)
print(X_PD_Encoded.shape)

X_Control_Encoded[0].shape

df = pd.DataFrame(columns=["Dim1", "Dim2", "Dim3", "Dim4", "Dim5", "Dim6", "Dim7", "Dim8", "Class"])

df

import pandas as pd

df = pd.DataFrame(columns=["Dim1", "Dim2", "Dim3", "Dim4", "Dim5", "Dim6", "Dim7", "Dim8", "Class"])

# Get the sizes of X_Control_Encoded and X_PD_Encoded
size0 = X_Control_Encoded.shape[0]
size1 = X_PD_Encoded.shape[0]

# Loop through the combined sizes of X_Control_Encoded and X_PD_Encoded
for i in range(size0+size1):
    if i < size0:
        # Assigning Control = 0
        row = X_Control_Encoded[i].tolist() + [0]
        df.loc[i] = row
    else:
        # Assigning PD = 1
        row = X_PD_Encoded[i - size0].tolist() + [1]
        df.loc[i] = row

# Ensure that the length of the DataFrame is equal to the combined sizes of X_Control_Encoded and X_PD_Encoded
assert len(df) == size0 + size1

df

df = df.sample(frac=1.0, random_state=12)

df

df.to_csv("Reduced_Dimension_AE_Dataset.csv")

import os
os.getcwd()

"""Saving at desired Google drive location"""

def changeDirectory(path):
    import os

    # Store the current working directory
    original_path = os.getcwd()

    # Change the directory to the specified path
    os.chdir(path)

    # Get the new working directory
    new_path = os.getcwd()

    print("Original path:", original_path)
    print("New path:", new_path)

changeDirectory("/content/drive/MyDrive/PhD/2023/PD/Paper 1/Reduced_dimension_AE")

df.to_csv("Reduced_Dimension_AE_Dataset.csv")